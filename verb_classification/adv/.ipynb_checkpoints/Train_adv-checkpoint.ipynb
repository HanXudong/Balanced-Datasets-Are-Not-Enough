{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, os, random, json, pickle, sys, pdb\n",
    "import string, shutil, time, argparse\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm as tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Function\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from data_loader import ImSituVerbGender\n",
    "from adv_model import VerbClassificationAdv\n",
    "from logger import Logger\n",
    "\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_checkpoint(args, state, is_best, filename):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, os.path.join(args.save_dir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def train(args, epoch, model, criterion, train_loader, optimizer, \\\n",
    "        train_logger, logging=True):\n",
    "\n",
    "    # set the training model\n",
    "    model.train()\n",
    "    nProcessed = 0\n",
    "    task_preds, adv_preds = [], []\n",
    "    task_truth, adv_truth = [], []\n",
    "    nTrain = len(train_loader.dataset) # number of images\n",
    "    task_loss_logger = AverageMeter()\n",
    "    adv_loss_logger = AverageMeter()\n",
    "\n",
    "\n",
    "    t = tqdm(train_loader, desc = 'Train %d' % epoch)\n",
    "    for batch_idx, (images, targets, genders, image_ids) in enumerate(t):\n",
    "        #if batch_idx == 100: break # constrain epoch size\n",
    "\n",
    "        # Set mini-batch dataset\n",
    "        if args.batch_balanced:\n",
    "            man_idx = genders[:, 0].nonzero().squeeze()\n",
    "            if len(man_idx.size()) == 0: man_idx = man_idx.unsqueeze(0)\n",
    "            woman_idx = genders[:, 1].nonzero().squeeze()\n",
    "            if len(woman_idx.size()) == 0: woman_idx = woman_idx.unsqueeze(0)\n",
    "            selected_num = min(len(man_idx), len(woman_idx))\n",
    "\n",
    "            if selected_num < args.batch_size / 2:\n",
    "                continue\n",
    "            else:\n",
    "                selected_num = args.batch_size / 2\n",
    "                selected_idx = torch.cat((man_idx[:selected_num], woman_idx[:selected_num]), 0)\n",
    "\n",
    "            images = torch.index_select(images, 0, selected_idx)\n",
    "            targets = torch.index_select(targets, 0, selected_idx)\n",
    "            genders = torch.index_select(genders, 0, selected_idx)\n",
    "\n",
    "        images = images.cuda()\n",
    "        targets = targets.cuda()\n",
    "        genders = genders.cuda()\n",
    "\n",
    "        # Forward, Backward and Optimizer\n",
    "        task_pred, adv_pred = model(images)\n",
    "\n",
    "        task_loss = criterion(task_pred, targets.max(1, keepdim=False)[1])\n",
    "        adv_loss = F.cross_entropy(adv_pred, genders.max(1, keepdim=False)[1], reduction='mean')\n",
    "\n",
    "        task_loss_logger.update(task_loss.item())\n",
    "        adv_loss_logger.update(adv_loss.item())\n",
    "\n",
    "        adv_pred = np.argmax(F.softmax(adv_pred, dim=1).cpu().detach().numpy(), axis=1)\n",
    "        adv_preds += adv_pred.tolist()\n",
    "        adv_truth += genders.cpu().max(1, keepdim=False)[1].numpy().tolist()\n",
    "\n",
    "        task_pred = F.softmax(task_pred, dim=1)\n",
    "        if batch_idx > 0 and len(task_preds) > 0:\n",
    "            task_preds = torch.cat((task_preds, task_pred.detach().cpu()), 0)\n",
    "            task_truth = torch.cat((task_truth, targets.cpu()), 0)\n",
    "            total_genders = torch.cat((total_genders, genders.cpu()), 0)\n",
    "        else:\n",
    "            task_preds = task_pred.detach().cpu()\n",
    "            task_truth = targets.cpu()\n",
    "            total_genders = genders.cpu()\n",
    "\n",
    "        loss = task_loss + adv_loss\n",
    "\n",
    "        # backpropogation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    task_f1_score = f1_score(task_truth.max(1)[1].numpy(), task_preds.max(1)[1].numpy(), average = 'macro')\n",
    "\n",
    "    man_idx = total_genders[:, 0].nonzero().squeeze()\n",
    "    woman_idx = total_genders[:, 1].nonzero().squeeze()\n",
    "    preds_man = torch.index_select(task_preds, 0, man_idx)\n",
    "    preds_woman = torch.index_select(task_preds, 0, woman_idx)\n",
    "    targets_man = torch.index_select(task_truth, 0, man_idx)\n",
    "    targets_woman = torch.index_select(task_truth, 0, woman_idx)\n",
    "    meanAP = average_precision_score(task_truth.numpy(), task_preds.numpy(), average='macro')\n",
    "    meanAP_man = average_precision_score(targets_man.numpy(), preds_man.numpy(), average='macro')\n",
    "    meanAP_woman = average_precision_score(targets_woman.numpy(), preds_woman.numpy(), average='macro')\n",
    "    adv_acc = accuracy_score(adv_truth, adv_preds)\n",
    "\n",
    "    if logging:\n",
    "        train_logger.scalar_summary('task loss', task_loss_logger.avg, epoch)\n",
    "        train_logger.scalar_summary('adv loss', adv_loss_logger.avg, epoch)\n",
    "        train_logger.scalar_summary('task_f1_score', task_f1_score, epoch)\n",
    "        train_logger.scalar_summary('meanAP', meanAP, epoch)\n",
    "        train_logger.scalar_summary('meanAP_man', meanAP_man, epoch)\n",
    "        train_logger.scalar_summary('meanAP_woman', meanAP_woman, epoch)\n",
    "        train_logger.scalar_summary('adv acc', adv_acc, epoch)\n",
    "\n",
    "    print('man size: {} woman size: {}'.format(len(man_idx), len(woman_idx)))\n",
    "    print('Train epoch  : {}, meanAP: {:.2f}, meanAP_man: {:.2f}, meanAP_woman: {:.2f}, adv acc: {:.2f}, '.format( \\\n",
    "        epoch, meanAP*100, meanAP_man*100, meanAP_woman*100, adv_acc*100))\n",
    "\n",
    "def test_balanced(args, epoch, model, criterion, val_loader, val_logger, print_every=10000, logging=True):\n",
    "\n",
    "    # set eval\n",
    "    model.eval()\n",
    "    nProcessed = 0\n",
    "    task_preds, adv_preds = [], []\n",
    "    task_truth, adv_truth = [], []\n",
    "    nTest = len(val_loader.dataset) # number of images\n",
    "    task_loss_logger = AverageMeter()\n",
    "    adv_loss_logger = AverageMeter()\n",
    "\n",
    "    t = tqdm(val_loader, desc = 'Val balanced %d' % epoch)\n",
    "    for batch_idx, (images, targets, genders, image_ids) in enumerate(t):\n",
    "        #if batch_idx == 100: break # constrain epoch size\n",
    "\n",
    "        # Set mini-batch dataset\n",
    "        images = images.cuda()\n",
    "        targets = targets.cuda()\n",
    "        genders = genders.cuda()\n",
    "\n",
    "        # Forward, Backward and Optimizer\n",
    "        task_pred, adv_pred = model(images)\n",
    "\n",
    "        task_loss = criterion(task_pred, targets.max(1, keepdim=False)[1])\n",
    "        adv_loss = F.cross_entropy(adv_pred, genders.max(1, keepdim=False)[1], reduction='mean')\n",
    "\n",
    "        task_loss_logger.update(task_loss.item())\n",
    "        adv_loss_logger.update(adv_loss.item())\n",
    "\n",
    "        adv_pred = np.argmax(F.softmax(adv_pred, dim=1).cpu().detach().numpy(), axis=1)\n",
    "        adv_preds += adv_pred.tolist()\n",
    "        adv_truth += genders.cpu().max(1, keepdim=False)[1].numpy().tolist()\n",
    "\n",
    "        task_pred = F.softmax(task_pred, dim=1)\n",
    "        if batch_idx > 0 and len(task_preds) > 0:\n",
    "            task_preds = torch.cat((task_preds, task_pred.detach().cpu()), 0)\n",
    "            task_truth = torch.cat((task_truth, targets.cpu()), 0)\n",
    "            total_genders = torch.cat((total_genders, genders.cpu()), 0)\n",
    "        else:\n",
    "            task_preds = task_pred.detach().cpu()\n",
    "            task_truth = targets.cpu()\n",
    "            total_genders = genders.cpu()\n",
    "\n",
    "        loss = task_loss + adv_loss\n",
    "\n",
    "    task_f1_score = f1_score(task_truth.max(1)[1].numpy(), task_preds.max(1)[1].numpy(), average = 'macro')\n",
    "\n",
    "    man_idx = total_genders[:, 0].nonzero().squeeze()\n",
    "    woman_idx = total_genders[:, 1].nonzero().squeeze()\n",
    "    preds_man = torch.index_select(task_preds, 0, man_idx)\n",
    "    preds_woman = torch.index_select(task_preds, 0, woman_idx)\n",
    "    targets_man = torch.index_select(task_truth, 0, man_idx)\n",
    "    targets_woman = torch.index_select(task_truth, 0, woman_idx)\n",
    "    meanAP = average_precision_score(task_truth.numpy(), task_preds.numpy(), average='macro')\n",
    "    meanAP_man = average_precision_score(targets_man.numpy(), preds_man.numpy(), average='macro')\n",
    "    meanAP_woman = average_precision_score(targets_woman.numpy(), preds_woman.numpy(), average='macro')\n",
    "    adv_acc = accuracy_score(adv_truth, adv_preds)\n",
    "\n",
    "    if logging:\n",
    "        val_logger.scalar_summary('adv loss balanced', adv_loss_logger.avg, epoch)\n",
    "        val_logger.scalar_summary('adv acc balanced', adv_acc, epoch)\n",
    "\n",
    "    print('man size: {} woman size: {}'.format(len(man_idx), len(woman_idx)))\n",
    "    print('Test epoch(f): {}, meanAP: {:.2f}, meanAP_man: {:.2f}, meanAP_woman: {:.2f}, adv acc: {:.2f}, '.format( \\\n",
    "        epoch, meanAP*100, meanAP_man*100, meanAP_woman*100, adv_acc*100))\n",
    "\n",
    "    return task_f1_score\n",
    "\n",
    "def test(args, epoch, model, criterion, val_loader, val_logger, print_every=10000, logging=True):\n",
    "    model.eval()\n",
    "    nProcessed = 0\n",
    "    task_preds, adv_preds = [], []\n",
    "    task_truth, adv_truth = [], []\n",
    "    nTest = len(val_loader.dataset) # number of images\n",
    "    task_loss_logger = AverageMeter()\n",
    "    adv_loss_logger = AverageMeter()\n",
    "\n",
    "    t = tqdm(val_loader, desc = 'Val %d' % epoch)\n",
    "    for batch_idx, (images, targets, genders, image_ids) in enumerate(t):\n",
    "        #if batch_idx == 100: break # constrain epoch size\n",
    "\n",
    "        # Set mini-batch dataset\n",
    "        images = images.cuda()\n",
    "        targets = targets.cuda()\n",
    "        genders = genders.cuda()\n",
    "\n",
    "        # Forward, Backward and Optimizer\n",
    "        task_pred, adv_pred = model(images)\n",
    "\n",
    "        task_loss = criterion(task_pred, targets.max(1, keepdim=False)[1])\n",
    "        adv_loss = F.cross_entropy(adv_pred, genders.max(1, keepdim=False)[1], reduction='mean')\n",
    "\n",
    "        task_loss_logger.update(task_loss.item())\n",
    "        adv_loss_logger.update(adv_loss.item())\n",
    "\n",
    "        adv_pred = np.argmax(F.softmax(adv_pred, dim=1).cpu().detach().numpy(), axis=1)\n",
    "        adv_preds += adv_pred.tolist()\n",
    "        adv_truth += genders.cpu().max(1, keepdim=False)[1].numpy().tolist()\n",
    "\n",
    "        task_pred = F.softmax(task_pred, dim=1)\n",
    "        if batch_idx > 0 and len(task_preds) > 0:\n",
    "            task_preds = torch.cat((task_preds, task_pred.detach().cpu()), 0)\n",
    "            task_truth = torch.cat((task_truth, targets.cpu()), 0)\n",
    "            total_genders = torch.cat((total_genders, genders.cpu()), 0)\n",
    "        else:\n",
    "            task_preds = task_pred.detach().cpu()\n",
    "            task_truth = targets.cpu()\n",
    "            total_genders = genders.cpu()\n",
    "\n",
    "        loss = task_loss + adv_loss\n",
    "\n",
    "    task_f1_score = f1_score(task_truth.max(1)[1].numpy(), task_preds.max(1)[1].numpy(), average = 'macro')\n",
    "\n",
    "    man_idx = total_genders[:, 0].nonzero().squeeze()\n",
    "    woman_idx = total_genders[:, 1].nonzero().squeeze()\n",
    "    preds_man = torch.index_select(task_preds, 0, man_idx)\n",
    "    preds_woman = torch.index_select(task_preds, 0, woman_idx)\n",
    "    targets_man = torch.index_select(task_truth, 0, man_idx)\n",
    "    targets_woman = torch.index_select(task_truth, 0, woman_idx)\n",
    "    meanAP = average_precision_score(task_truth.numpy(), task_preds.numpy(), average='macro')\n",
    "    meanAP_man = average_precision_score(targets_man.numpy(), preds_man.numpy(), average='macro')\n",
    "    meanAP_woman = average_precision_score(targets_woman.numpy(), preds_woman.numpy(), average='macro')\n",
    "    adv_acc = accuracy_score(adv_truth, adv_preds)\n",
    "\n",
    "    if logging:\n",
    "        val_logger.scalar_summary('task loss', task_loss_logger.avg, epoch)\n",
    "        val_logger.scalar_summary('adv loss', adv_loss_logger.avg, epoch)\n",
    "        val_logger.scalar_summary('task_f1_score', task_f1_score, epoch)\n",
    "        val_logger.scalar_summary('meanAP', meanAP, epoch)\n",
    "        val_logger.scalar_summary('meanAP_man', meanAP_man, epoch)\n",
    "        val_logger.scalar_summary('meanAP_woman', meanAP_woman, epoch)\n",
    "        val_logger.scalar_summary('adv acc', adv_acc, epoch)\n",
    "\n",
    "    print('man size: {} woman size: {}'.format(len(man_idx), len(woman_idx)))\n",
    "    print('Test epoch   : {}, meanAP: {:.2f}, meanAP_man: {:.2f}, meanAP_woman: {:.2f}, adv acc: {:.2f}, '.format( \\\n",
    "        epoch, meanAP*100, meanAP_man*100, meanAP_woman*100, adv_acc*100))\n",
    "\n",
    "    return task_f1_score\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arg:\n",
    "    exp_id =  \"exp_id\"\n",
    "    log_dir = \"log_dir\"\n",
    "    ratio = \"0\"\n",
    "    num_verb = 211\n",
    "    annotation_dir = \"../data\"\n",
    "    image_dir = \"../data/of500_images_resized\"\n",
    "    balanced = False\n",
    "    gender_balanced = False\n",
    "    batch_balanced = False\n",
    "    no_image = False\n",
    "    adv_on = False\n",
    "    layer = None\n",
    "    adv_conv = False\n",
    "    no_avgpool = False\n",
    "    adv_capacity = 0\n",
    "    adv_lambda = 1e-0\n",
    "    dropout = 1e-1\n",
    "    blackout = False\n",
    "    blackout_box = False\n",
    "    blackout_face = False\n",
    "    blur = False\n",
    "    grayscale = False\n",
    "    edges = False\n",
    "    resume = False\n",
    "    learning_rate = 1e-4\n",
    "    finetune = False\n",
    "    num_epochs = 50\n",
    "    batch_size = 64\n",
    "    crop_size = 224\n",
    "    image_size = 256\n",
    "    start_epoch = 1\n",
    "    seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Arg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.exp_id = \"Test_ADV-Off_Layers-conv4\"\n",
    "args.adv_on = False\n",
    "args.layer = \"conv4\"\n",
    "args.no_avgpool = True\n",
    "args.adv_capacity = 300\n",
    "args.adv_lambda = 1\n",
    "args.learning_rate = 0.00005\n",
    "args.num_epochs = 100\n",
    "args.batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model save directory\n",
    "args.save_dir = os.path.join('./models', args.layer + '_' + str(args.adv_capacity) + '_' + \\\n",
    "        str(args.adv_lambda) + '_' + str(args.dropout) + '_' + args.exp_id)\n",
    "if not os.path.exists(args.save_dir): os.makedirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Project\\Balanced-Datasets-Are-Not-Enough\\verb_classification\\adv\\logger.py:15: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create log save directory\n",
    "args.log_dir = os.path.join('./logs', args.layer + '_' + str(args.adv_capacity) + '_' + \\\n",
    "        str(args.adv_lambda) + '_' + str(args.dropout) + '_' + args.exp_id)\n",
    "train_log_dir = os.path.join(args.log_dir, 'train')\n",
    "val_log_dir = os.path.join(args.log_dir, 'val')\n",
    "if not os.path.exists(train_log_dir): os.makedirs(train_log_dir)\n",
    "if not os.path.exists(val_log_dir): os.makedirs(val_log_dir)\n",
    "train_logger = Logger(train_log_dir)\n",
    "val_logger = Logger(val_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all parameters for training\n",
    "with open(os.path.join(args.log_dir, \"arguments.txt\"), \"a\") as f:\n",
    "    f.write(str(args)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean = [0.485, 0.456, 0.406],std = [0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(args.image_size),\n",
    "    transforms.RandomCrop(args.crop_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(args.image_size),\n",
    "    transforms.CenterCrop(args.crop_size),\n",
    "    transforms.ToTensor(),\n",
    "    normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImSituVerbGender dataloader\n",
      "loading train annotations..........\n",
      "dataset size: 24301\n",
      "man size : 14199 and woman size: 10102\n",
      "ImSituVerbGender dataloader\n",
      "loading val annotations..........\n",
      "dataset size: 7730\n",
      "man size : 4457 and woman size: 3273\n",
      "ImSituVerbGender dataloader\n",
      "loading val annotations..........\n",
      "dataset size: 7730\n",
      "man size : 3000 and woman size: 3000\n"
     ]
    }
   ],
   "source": [
    "# Data samplers.\n",
    "train_data = ImSituVerbGender(args,\n",
    "                              annotation_dir = args.annotation_dir,\n",
    "                              image_dir = args.image_dir,\n",
    "                              split = 'train',\n",
    "                              transform = train_transform)\n",
    "val_data = ImSituVerbGender(args,\n",
    "                            annotation_dir = args.annotation_dir,\n",
    "                            image_dir = args.image_dir,\n",
    "                            split = 'val',\n",
    "                            transform = val_transform)\n",
    "args.gender_balanced = True\n",
    "val_data_gender_balanced = ImSituVerbGender(args, \n",
    "                                            annotation_dir = args.annotation_dir,\n",
    "                                            image_dir = args.image_dir,\n",
    "                                            split = 'val',\n",
    "                                            transform = val_transform)\n",
    "args.gender_balanced = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders / batch assemblers.\n",
    "if args.batch_balanced:\n",
    "    train_batch_size = int(2.5 * args.batch_size)\n",
    "else:\n",
    "    train_batch_size = int(args.batch_size)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = train_batch_size,\n",
    "        shuffle = True, num_workers = 0, pin_memory = True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size = args.batch_size,\n",
    "        shuffle = False, num_workers = 0, pin_memory = True)\n",
    "val_loader_gender_balanced = torch.utils.data.DataLoader(val_data_gender_balanced, \\\n",
    "    batch_size = args.batch_size, shuffle = False, num_workers = 0, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build a VerbClassification Model[conv4]\n",
      "Load weights from Resnet18/50 done\n"
     ]
    }
   ],
   "source": [
    "    # Build the models\n",
    "model = VerbClassificationAdv(args, \n",
    "                              args.num_verb,\n",
    "                              args.adv_capacity,\n",
    "                              args.dropout,\n",
    "                              args.adv_lambda).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build loss\n",
    "verb_weights = torch.FloatTensor(train_data.getVerbWeights())\n",
    "criterion = nn.CrossEntropyLoss(weight=verb_weights, reduction='mean').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_trainable_params: 60826841\n"
     ]
    }
   ],
   "source": [
    "# build optimizer\n",
    "def trainable_params():\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            yield param\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('num_trainable_params:', num_trainable_params)\n",
    "optimizer = torch.optim.Adam(trainable_params(), args.learning_rate, weight_decay = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_performance = 0\n",
    "if args.resume:\n",
    "    if os.path.isfile(os.path.join(args.save_dir, 'checkpoint.pth.tar')):\n",
    "        print(\"=> loading checkpoint '{}'\".format(args.save_dir))\n",
    "        checkpoint = torch.load(os.path.join(args.save_dir, 'checkpoint.pth.tar'))\n",
    "        args.start_epoch = checkpoint['epoch']\n",
    "        best_performance = checkpoint['best_performance']\n",
    "        # load partial weights\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in checkpoint['state_dict'].items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "        print(\"=> loaded checkpoint (epoch {})\".format(checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(args.save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before training, evaluate the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "906e702909514d1e936050821e2d2492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Val balanced 0', max=47.0, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emano\\Anaconda3\\envs\\py37-Torch-Transformers\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:677: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man size: 3000 woman size: 3000\n",
      "Test epoch(f): 0, meanAP: 0.73, meanAP_man: nan, meanAP_woman: nan, adv acc: 50.00, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emano\\Anaconda3\\envs\\py37-Torch-Transformers\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:677: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40eacf0669944cdeb4966c0a11a197b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Val 0', max=61.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "man size: 4457 woman size: 3273\n",
      "Test epoch   : 0, meanAP: 0.69, meanAP_man: 0.73, meanAP_woman: nan, adv acc: 57.66, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emano\\Anaconda3\\envs\\py37-Torch-Transformers\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:677: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.001921062242070278"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('before training, evaluate the model')\n",
    "test_balanced(args, 0, model, criterion, val_loader_gender_balanced,\n",
    "    val_logger, logging=False)\n",
    "test(args, 0, model, criterion, val_loader, val_logger, logging=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(args.start_epoch, args.num_epochs + 1):\n",
    "    train(args, epoch, model, criterion, train_loader, optimizer, train_logger, logging=True)\n",
    "    test_balanced(args, epoch, model, criterion, val_loader_gender_balanced,\n",
    "        val_logger, logging=True)\n",
    "    current_performance = test(args, epoch, model, criterion, val_loader, val_logger, logging = True)\n",
    "    is_best = current_performance > best_performance\n",
    "    best_performance = max(current_performance, best_performance)\n",
    "    model_state = {\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_performance': best_performance}\n",
    "    save_checkpoint(args, model_state, is_best, os.path.join(args.save_dir, 'checkpoint.pth.tar'))\n",
    "    # at the end of every run, save the model\n",
    "    if epoch ==  args.num_epochs:\n",
    "        torch.save(model_state, os.path.join(args.save_dir, 'checkpoint_%s.pth.tar' % str(args.num_epochs)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('py37-Torch-Transformers': conda)",
   "language": "python",
   "name": "python37664bitpy37torchtransformerscondae93daae8d17e423a9d2b476995b931c9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
